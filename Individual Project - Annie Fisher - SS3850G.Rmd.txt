---
title: "3850G Individual Project - Heart Disease Data Analysis "
author: "Annie Fisher"
date: "05/03/2020"
output: 
  html_document:
    toc: true
    number_sections: true
  
---
# Executive Summary 
This report summarizes the statistical modeling and analysis of data associated with the study and documentation of heart disease. The purpose of this report is to document the implemented sampling design and the corresponding statistical modeling and inferencing techniques used during the analysis of the data. It is aimed for this report to learn and attempt to:
  
  1.) Place statistical analysis in the framework of research questions and hypotheses
  
  2.) Using R and statistical concepts learnt in class to determine seletions of promising modelling approaches 
  
  3.) Visualisation and correct interpretations data
  
  
This report follows the data analysis of a typical medical science problem proceeding from visulations and exploration throguh regression analysis, bivariate correlation, analysis of variance and multivariate factor analysis. It attemps to analyze risk facotrs of heart disease and identify the most important factors. 
  
  
This report attemps to answer the following questions:

  1.) Why are some individuals more suseptible then and others?
  
  2.) What factors are the most influencial in getting heart disease?
  
  3.) What potential category factors (demographic, behavior and medical risk factors) are more detrimental?
  
  4.) What is the inter-relation (association, correlation) between these three
variables? How much total information do they provide?

  5.) What predictors are the most important for prediction model? Analysis model?
  
  6.) What trends does the data follow? 

  
# Introduction 
This data set was orginally prepared in a excel spreedsheet and exported as a text file named framingham.csv. It was taken from public dataset website Kaggle and documents the ongoing cardiovascular study on residents of the town Farmingham, Massachusetts. 

The classification goal of this study is to predict whether the patients have a 10 year risk of future coronary heart disease (CHD). The dataset includes over 4,000 records of patients and has 15 attributes that have been recorded as possible influences in the risk of CHD. The attribute variables include potential risk factors such as demographic, behavior and medical risk factors. 

In this report I have used these statistical methods & analyses:

- Logistic Regression
- LDA
- Tree Classification
- Bagging
- Model Selection AIC
- and more!

and these packages:

- library(dvmisc)
- library(knitr)
- library(ggplot2)
- library(ISLR)
- library(lmtest)
- library(e1071)
- library(caret)
- library(pROC)
- library(MASS)
- library(randomForest)
- library(tree)
- and more!


# Overview of the Dataset
```{r}
heartDiesease_data <- read.csv("C:/Users/Owner/Desktop/3850B/Individual Project Files/framingham.csv")
predictors <- names(heartDiesease_data)
str(heartDiesease_data)


#Ploting relationships between coefficients 
plot(heartDiesease_data)
abline(heartDiesease_data, col = "darkOrange")
```

# Univariate Analysis 

Here we consider the data looking at certain varaible sepreately. I have chosen to look at sex, age, currentSmoker, BMI and totChol attributes as potential important/high risk factors in the TenYearCHD risk. In addition, I have chosen these attributes as they are the most commonly known the the general public. 

```{r}
# dataset containing all the attributes of interest (sex, age, currentSmoker, BMI andtotChol) in relation to TenYearCHD 
univ_data <- heartDiesease_data[, c(1,2,4,13,16)]  

summary(univ_data)
```

# Sex as a Risk Factor

It is uncertain as an initial hypothesis wether sex would have a significance in the risk of developping heart disease.

```{r}
sex_data <- univ_data[,1]
sex_data <- as.factor(sex_data)
par(mfrow=c(1,2))
barplot(table(sex_data))
```

We can see that the majority of the patients in this study are female, however the difference is not that significant.

```{r}
heartDiesease_data$male <-factor(heartDiesease_data$male)

heartDiesease_data$TenYearCHD <- factor(heartDiesease_data$TenYearCHD)
fit_sex <- lm(TenYearCHD~male, data= heartDiesease_data)
summary(fit_sex)$coef


fit_glm = glm(TenYearCHD~male, data= heartDiesease_data, family = binomial(link = "logit"))
n = nrow(heartDiesease_data)
y_hat = rep(0,n)

plot(TenYearCHD ~ male, data = heartDiesease_data, pch = 20, ylab = "Probability of Heart Disease", xlab =" Sex (0=Female, 1=Male)")

```

From the output above, it can be determined that in this study more males that the risk of getting heart disease is higher for males than it is for females. The P-value for the dummy variable male is very significant, suggeating that there is a statistical evidence of a difference in risk of getting heart disease between genders.

From the plot, we see the widths of the columns correspond to the number of patients in each of that have a 10 year risk of coronary heart disease (binary: “1”, means “Yes”, “0” means “No”).

# Age as a Risk Factor

It is hypothesized that the older the patient would result in a greater risk in developping heart disease. 

```{r}
age_data <- univ_data[,2]
par(mfrow=c(1,2))
barplot(table(age_data))

mean(age_data)
```

We can see that the patietns in this study range from 30-70 years old with the average age of around 50 years old. 

```{r}
fit_age = glm(TenYearCHD~age, data= heartDiesease_data, family = binomial(link = "logit"))
summary(fit_age)$coef

plot(TenYearCHD ~ age, data = heartDiesease_data, pch = 20, ylab = "Probability of Heart Disease", xlab = "age")

qqnorm(heartDiesease_data$age, main="QQ Plot for Age vs Normal Distribution", ylab = "Age")
qqline(heartDiesease_data$age)

qqnorm(log(heartDiesease_data$age), main="QQ Plot for Age vs Lognormal Distribution", ylab = "Age")
qqline(log(heartDiesease_data$age))


hist(heartDiesease_data$age, freq=F, breaks=seq(0,100,5),  main="Probability density for Age")
lines(density(heartDiesease_data$age),lwd=2)
lines(density(heartDiesease_data$age, adj=.5),lwd=1)
lines(density(heartDiesease_data$age, adj=2),lwd=1.5)

boxplot(heartDiesease_data$age, notch=T, horizontal=T,main="Boxplot of Age")
```

From the graph above, we can see that age does seem to have a significant relationship with heart disease. It is observed that older the patient is, the more at risk they are in developping heart disease. 


# Current Smoker as a Risk Factor

It is hypothesized that smoking would result in a greater risk in developping heart disease.

```{r}
smoker_data <- univ_data[,3]
smoker_data <- as.factor(smoker_data)
par(mfrow=c(1,2))
barplot(table(smoker_data))
```

The amount of smokers and non smokers is close to equal in this study.

```{r}
heartDiesease_data$currentSmoker <-factor(heartDiesease_data$currentSmoker)
fit_smoker = glm(TenYearCHD~ currentSmoker, data= heartDiesease_data, family = binomial(link = "logit"))
summary(fit_smoker)$coef

plot(TenYearCHD ~ currentSmoker, data = heartDiesease_data, pch = 20, ylab = "Probability of Heart Disease", xlab = "Current Smoker")
```

From the output above, we can see from the P-value for the dummy variable currentSmoker is very significant, suggesting that there is a statistical evidence of a difference in risk of getting heart disease between being a current smoking.


# BMI as a Risk Factor

It is hypothesized that the greater BMI would result in a greater risk in developping heart disease. 

```{r}
require(dvmisc)
bmi_data <- univ_data[,4]
par(mfrow=c(1,2))
barplot(table(bmi_data))

bmi_categories <- bmi4(bmi_data, labels = TRUE)
barplot(table(bmi_categories))
```

We can see that the patietns in this study range from BMI of 15-35 with the majority of the BMI's in the noraml-overweight categories.

BMI Categories:
- Underweight if [-Inf, 18.5)
- Normal weight if [18.5, 25)
- Overweight if [25, 30), a
- Obese if [30, Inf).


```{r}
heartDiesease_data$BMI <- bmi_categories
fit_bmi = glm(TenYearCHD~ BMI, data= heartDiesease_data, family = binomial(link = "logit"))
summary(fit_bmi)

plot(TenYearCHD ~ BMI, data = heartDiesease_data, pch = 20, ylab = "Probability of Heart Disease", xlab = "BMI")
```

From the graph above, we can see that BMI does seem to have a significant relationship with heart disease. It is observed that the opposite ends of the BMI spetrum, both underweight (< 15) and overweight & obese (> 25) does increase the risk of developping heart disease. 

# Multiple Regression Analysis

```{r}
library(knitr)
library(ggplot2)
library(ISLR)
library(lmtest)

numObs <- nrow(heartDiesease_data) #number of observations: numObs = 4238

full_model <- glm(TenYearCHD~., data=heartDiesease_data, family = binomial(link = "logit"))
summary(full_model)


plot(full_model,1)
plot(full_model,2)
plot(full_model,4)

```

The linearity assumption holds true, the red line is approximately horizontal at zero.

The normality assumption does not hold, distribution looks more lognormal


# Model Selection

```{r}
library(faraway)
heartDiesease_data <- read.csv("C:/Users/Owner/Desktop/3850B/Individual Project Files/framingham.csv")
heartDiesease_data <- na.omit(heartDiesease_data)
full_model <- glm(TenYearCHD~., data=heartDiesease_data, family = binomial(link = "logit"))
summary(full_model)

#AIC Backward Model Selection
fit_back_aic = step(full_model, direction = "backward")
summary(fit_back_aic)

#BIC Backward Model Selection
n = nrow(heartDiesease_data)
fit_back_bic = step(full_model, direction = "backward", k=log(n))
summary(fit_back_bic)

anova(fit_back_aic,fit_back_bic)
```

The model chosen by backward selection using the AIC metric has chosen a model with 8 predictors
formula = TenYearCHD ~ male + age + cigsPerDay + prevalentStroke + prevalentHyp + totChol + sysBP + glucose

while,

The model chosen by backward selection using the BIC metric has chosen a model with 5 predictors
formula = TenYearCHD ~ male + age + cigsPerDay + sysBP + glucose


The diffence in my approach then the approach listed in Kaggle is that they performed a backwards eliminaiton (P-value Approach) while I performed a forward selection using AIC and BIC

There model was:
Formula = TenYearCHD ~ male + age + cigsPerDay+ totChol + sysBP + logit(glucose)

This model include 6 predictors and is very closely related to both models from my model selection. It is less complex then my AIC mdoel and more complex then my BIC model

They found that the fitted model shows that, holding all other features constant, the odds of getting diagnosed with heart disease for males (sexmale = 1) over that of females (sexmale = 0) is exp(0.5815) = 1.788687. In terms of percent change, we can say that the odds for males are 78.8% higher than the odds for females.

The coefficient for age says that, holding all others constant, we will see 7% increase in the odds of getting diagnosed with CDH for a one year increase in age since exp(0.0655) = 1.067644.


# Comparison of Model Techniques 

```{r}

#fitted model given from kaggle
example_fit <- glm(TenYearCHD ~ male + age + cigsPerDay+ totChol + sysBP + log(glucose), data = heartDiesease_data, family = binomial(link = "logit"))

anova(example_fit, fit_back_aic, fit_back_bic,test="Chisq")
```

# Model Diagnostics

```{r}
library(e1071)
library(caret)
library(pROC)
library(MASS)

#Creating a new data set with only the predictors found above
new_heartData <- na.omit(heartDiesease_data)
names(new_heartData)

smp_size <- floor(0.50 * nrow(new_heartData))
train_ind <- sample(seq_len(nrow(new_heartData)), size = smp_size)
train <-new_heartData[train_ind,]
test <- new_heartData[-train_ind,]


model1 <-glm(TenYearCHD ~ male + age + cigsPerDay + prevalentStroke + prevalentHyp + totChol + sysBP + glucose, data=train, family = binomial)
summary(model1)
coef(model1)

train.mse<- mean(residuals(model1)^2)
train.mse
#The training MSE is 0.775345



log_prob<-predict(model1,test, type="response")
log.pred<-rep(0, length(log_prob))
log.pred[log_prob>0.5]=1
log.pred = as.factor(log.pred)
test$TenYearCHD = as.factor(test$TenYearCHD)
train$TenYearCHD = as.factor(train$TenYearCHD)

confusionMatrix(log.pred, test$TenYearCHD, positive = NULL, dnn = c("Prediction", "Reference"))


#     Accuracy : 0.8496  
#    Sensitivity : 0.99222         
#    Specificity : 0.07719     
```

We can see that from the accuracy of this model it is a good fit for the data. By running a logistic regression over the trainijng dataset and predicting the value of our response for the test dataset we are able to generate a confusion matrix that show a good value for Accuracy. 


# Linear Discriminant Analysis

```{r}
library(e1071)
library(caret)
library(pROC)
library(MASS)
LDA_fit <- lda(TenYearCHD ~ male + age + cigsPerDay + prevalentStroke + prevalentHyp + totChol + sysBP + glucose, data=train)
LDA_fit

LDA_pred <- predict(LDA_fit,test)$class
confusionMatrix(LDA_pred, test$TenYearCHD, positive = NULL, dnn = c("Prediction", "Reference"))

#   Accuracy : 0.8496 
```

We see that this model shows good accuracy performing a LDA, thus model strength holds



# Tree Classification Analysis

```{r}
library(tree)

tree =tree(TenYearCHD ~ male + age + cigsPerDay + prevalentStroke + prevalentHyp + totChol + sysBP + glucose,data =train)
summary(tree)
plot(tree)
text(tree)

tree_pred <- predict(tree, test, type="class")

confusionMatrix(tree_pred, test$TenYearCHD, positive = NULL, dnn = c("Prediction", "Reference"))

# Accuracy : 0.8441       
```

Error classification is low at 0.1488 which tells us the model strength is good

# Bagging

```{r}
library(randomForest)
bag<-randomForest(TenYearCHD ~ male + age + cigsPerDay + prevalentStroke + prevalentHyp + totChol + sysBP + glucose, data=train, mtry=5, importance = TRUE)
bag_pre<-predict(bag, newdata=test, type="class")

confusionMatrix(bag_pre, test$TenYearCHD, positive = NULL, dnn = c("Prediction", "Reference"))

#  Accuracy : 0.8408   
```

# CONCLUSION

I found that:

- More males that the risk of getting heart disease is higher for males than it is for females

- From the patietns in this study ranging from 30-70 years old with the average age of around 50 years old. Age does seem to have a significant relationship with heart disease. It is observed that older the patient is, the more at risk they are in developping heart disease.
  
- Being a current smoking increases the chances for the patient of developping heart disease

- BMI does seem to have a significant relationship with heart disease. It is observed that the opposite ends of the BMI spetrum, both underweight (< 15) and overweight & obese (> 25) does increase the risk of developping heart disease. 

- Looking at all predictors, by using backward selection using the AIC metric a model with 8 predictors: TenYearCHD ~ male + age + cigsPerDay + prevalentStroke + prevalentHyp + totChol + sysBP + glucose was chosen as the best. 

- By various regression analyses above it was varified that this model is indeed a good fit for the data.













